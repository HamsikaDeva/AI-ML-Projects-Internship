{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k5HPzf5cxe7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "books_df = pd.read_csv('/content/books.csv')\n",
        "\n",
        "def search_books(keyword):\n",
        "    \"\"\"\n",
        "    Search for books that match the keyword in the title, authors, or publisher.\n",
        "    \"\"\"\n",
        "    keyword = keyword.lower()\n",
        "    results = books_df[\n",
        "        books_df['title'].str.lower().str.contains(keyword) |\n",
        "        books_df['authors'].str.lower().str.contains(keyword) |\n",
        "        books_df['publisher'].str.lower().str.contains(keyword)\n",
        "    ]\n",
        "    return results\n",
        "\n",
        "\n",
        "def display_books(books):\n",
        "    \"\"\"\n",
        "    Display the list of books.\n",
        "    \"\"\"\n",
        "    if books.empty:\n",
        "        print(\"No books found.\")\n",
        "    else:\n",
        "        for index, book in books.iterrows():\n",
        "            print(f\"Title: {book['title']}\")\n",
        "            print(f\"Author: {book['authors']}\")\n",
        "            print(f\"Average Rating: {book['average_rating']}\")\n",
        "            print(f\"ISBN: {book['isbn']}\")\n",
        "            print(f\"ISBN13: {book['isbn13']}\")\n",
        "            print(f\"Language: {book['language_code']}\")\n",
        "            # Check if 'num_pages' column exists before printing\n",
        "            if 'num_pages' in book:\n",
        "                print(f\"Number of Pages: {book['num_pages']}\")\n",
        "            print(f\"Ratings Count: {book['ratings_count']}\")\n",
        "            print(f\"Text Reviews Count: {book['text_reviews_count']}\")\n",
        "            print(f\"Publication Date: {book['publication_date']}\")\n",
        "            print(f\"Publisher: {book['publisher']}\")\n",
        "            print('-' * 40)\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the Book Recommendation Program!\")\n",
        "    keyword = input(\"Enter a keyword to search for books: \")\n",
        "    results = search_books(keyword)\n",
        "    display_books(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDvG2SHtdEQJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlSTy187BHqF"
      },
      "outputs": [],
      "source": [
        "SEED_VALUE = 42\n",
        "\n",
        "# Fix seed to make training deterministic.\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "tf.random.set_seed(SEED_VALUE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBzgnVHlBIhs"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1w-JlX6BRiU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 9))\n",
        "\n",
        "num_rows = 4\n",
        "num_cols = 8\n",
        "\n",
        "# plot each of the images in the batch and the associated ground truth labels.\n",
        "for i in range(num_rows*num_cols):\n",
        "    ax = plt.subplot(num_rows, num_cols, i + 1)\n",
        "    plt.imshow(X_train[i,:,:])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoeG9ab5BUrE"
      },
      "outputs": [],
      "source": [
        "#  Normalize images to the range [0, 1].\n",
        "X_train = X_train.astype(\"float32\") / 255\n",
        "X_test  = X_test.astype(\"float32\") / 255\n",
        "\n",
        "# Change the labels from integer to categorical data.\n",
        "print('Original (integer) label for the first training sample: ', y_train[0])\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = to_categorical(y_train)\n",
        "y_test  = to_categorical(y_test)\n",
        "\n",
        "print('After conversion to categorical one-hot encoded labels: ', y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehKRN2YNBW9b"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class DatasetConfig:\n",
        "    NUM_CLASSES:  int = 10\n",
        "    IMG_HEIGHT:   int = 32\n",
        "    IMG_WIDTH:    int = 32\n",
        "    NUM_CHANNELS: int = 3\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "    EPOCHS:        int = 31\n",
        "    BATCH_SIZE:    int = 256\n",
        "    LEARNING_RATE: float = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OotVS-ALBcn8"
      },
      "outputs": [],
      "source": [
        "def cnn_model(input_shape=(32, 32, 3)):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 1: 32 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 2: 64 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 3: 64 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Flatten the convolutional features.\n",
        "    #------------------------------------\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYfrMGXrBdmz"
      },
      "outputs": [],
      "source": [
        "# Create the model.\n",
        "model = cnn_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KsXBVrQBlC7"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxQOTuUBnsT"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                    epochs=TrainingConfig.EPOCHS,\n",
        "                    verbose=1,\n",
        "                    validation_split=.3,\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED7WjtcmBwZ0"
      },
      "outputs": [],
      "source": [
        "def plot_results(metrics, title=None, ylabel=None, ylim=None, metric_name=None, color=None):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 4))\n",
        "\n",
        "    if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n",
        "        metrics = [metrics,]\n",
        "        metric_name = [metric_name,]\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax.plot(metric, color=color[idx])\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.xlim([0, TrainingConfig.EPOCHS-1])\n",
        "    plt.ylim(ylim)\n",
        "    # Tailor x-axis tick marks\n",
        "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
        "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
        "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "    plt.grid(True)\n",
        "    plt.legend(metric_name)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMbvaONWB0Vj"
      },
      "outputs": [],
      "source": [
        "# Retrieve training results.\n",
        "train_loss = history.history[\"loss\"]\n",
        "train_acc  = history.history[\"accuracy\"]\n",
        "valid_loss = history.history[\"val_loss\"]\n",
        "valid_acc  = history.history[\"val_accuracy\"]\n",
        "\n",
        "plot_results([ train_loss, valid_loss ],\n",
        "            ylabel=\"Loss\",\n",
        "            ylim = [0.0, 5.0],\n",
        "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
        "            color=[\"g\", \"b\"]);\n",
        "\n",
        "plot_results([ train_acc, valid_acc ],\n",
        "            ylabel=\"Accuracy\",\n",
        "            ylim = [0.0, 1.0],\n",
        "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
        "            color=[\"g\", \"b\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hekLgJQNB7Vj"
      },
      "outputs": [],
      "source": [
        "def cnn_model_dropout(input_shape=(32, 32, 3)):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 1: 32 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 2: 64 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Conv Block 3: 64 Filters, MaxPool.\n",
        "    #------------------------------------\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    #------------------------------------\n",
        "    # Flatten the convolutional features.\n",
        "    #------------------------------------\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANaiwDmvB8L7"
      },
      "outputs": [],
      "source": [
        "# Create the model.\n",
        "model_dropout = cnn_model_dropout()\n",
        "model_dropout.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5rDdK2eCA3z"
      },
      "outputs": [],
      "source": [
        "model_dropout.compile(optimizer='rmsprop',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'],\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lZW7jERCDCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ad4930-53e9-4933-cee3-494f4d213a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/31\n",
            "137/137 [==============================] - 237s 2s/step - loss: 2.1564 - accuracy: 0.2079 - val_loss: 1.8532 - val_accuracy: 0.3262\n",
            "Epoch 2/31\n",
            "137/137 [==============================] - 214s 2s/step - loss: 1.8608 - accuracy: 0.3230 - val_loss: 1.6470 - val_accuracy: 0.3912\n",
            "Epoch 3/31\n",
            "137/137 [==============================] - 231s 2s/step - loss: 1.6607 - accuracy: 0.3983 - val_loss: 1.4819 - val_accuracy: 0.4593\n",
            "Epoch 4/31\n",
            "137/137 [==============================] - 227s 2s/step - loss: 1.5022 - accuracy: 0.4594 - val_loss: 1.3843 - val_accuracy: 0.4981\n",
            "Epoch 5/31\n",
            "137/137 [==============================] - 230s 2s/step - loss: 1.3858 - accuracy: 0.5015 - val_loss: 1.2718 - val_accuracy: 0.5304\n",
            "Epoch 6/31\n",
            "137/137 [==============================] - 216s 2s/step - loss: 1.2752 - accuracy: 0.5414 - val_loss: 1.2410 - val_accuracy: 0.5494\n",
            "Epoch 7/31\n",
            "137/137 [==============================] - 216s 2s/step - loss: 1.1878 - accuracy: 0.5764 - val_loss: 1.1065 - val_accuracy: 0.6093\n",
            "Epoch 8/31\n",
            "137/137 [==============================] - 233s 2s/step - loss: 1.1101 - accuracy: 0.6067 - val_loss: 1.0061 - val_accuracy: 0.6387\n",
            "Epoch 9/31\n",
            "137/137 [==============================] - 234s 2s/step - loss: 1.0542 - accuracy: 0.6299 - val_loss: 0.9648 - val_accuracy: 0.6569\n",
            "Epoch 10/31\n",
            "137/137 [==============================] - 234s 2s/step - loss: 0.9936 - accuracy: 0.6485 - val_loss: 0.8891 - val_accuracy: 0.6867\n",
            "Epoch 11/31\n",
            "137/137 [==============================] - 234s 2s/step - loss: 0.9487 - accuracy: 0.6646 - val_loss: 0.9046 - val_accuracy: 0.6809\n",
            "Epoch 12/31\n",
            "137/137 [==============================] - 232s 2s/step - loss: 0.9069 - accuracy: 0.6797 - val_loss: 0.8816 - val_accuracy: 0.6874\n",
            "Epoch 13/31\n",
            "137/137 [==============================] - 231s 2s/step - loss: 0.8585 - accuracy: 0.6961 - val_loss: 0.9807 - val_accuracy: 0.6650\n",
            "Epoch 14/31\n",
            "137/137 [==============================] - 233s 2s/step - loss: 0.8317 - accuracy: 0.7077 - val_loss: 0.8726 - val_accuracy: 0.6990\n",
            "Epoch 15/31\n",
            "137/137 [==============================] - 233s 2s/step - loss: 0.7982 - accuracy: 0.7198 - val_loss: 0.9093 - val_accuracy: 0.6801\n",
            "Epoch 16/31\n",
            "137/137 [==============================] - 215s 2s/step - loss: 0.7679 - accuracy: 0.7301 - val_loss: 0.7300 - val_accuracy: 0.7430\n",
            "Epoch 17/31\n",
            "137/137 [==============================] - 217s 2s/step - loss: 0.7370 - accuracy: 0.7413 - val_loss: 0.7537 - val_accuracy: 0.7383\n",
            "Epoch 18/31\n",
            "137/137 [==============================] - 234s 2s/step - loss: 0.7168 - accuracy: 0.7490 - val_loss: 0.6964 - val_accuracy: 0.7555\n",
            "Epoch 19/31\n",
            "137/137 [==============================] - 236s 2s/step - loss: 0.6861 - accuracy: 0.7577 - val_loss: 0.7407 - val_accuracy: 0.7422\n",
            "Epoch 20/31\n",
            "137/137 [==============================] - 232s 2s/step - loss: 0.6608 - accuracy: 0.7657 - val_loss: 0.7270 - val_accuracy: 0.7497\n",
            "Epoch 21/31\n",
            "137/137 [==============================] - 234s 2s/step - loss: 0.6412 - accuracy: 0.7730 - val_loss: 0.7007 - val_accuracy: 0.7625\n",
            "Epoch 22/31\n",
            "137/137 [==============================] - 233s 2s/step - loss: 0.6177 - accuracy: 0.7826 - val_loss: 0.6802 - val_accuracy: 0.7654\n",
            "Epoch 23/31\n",
            "137/137 [==============================] - 232s 2s/step - loss: 0.6028 - accuracy: 0.7866 - val_loss: 0.6573 - val_accuracy: 0.7746\n",
            "Epoch 24/31\n",
            "137/137 [==============================] - 233s 2s/step - loss: 0.5818 - accuracy: 0.7922 - val_loss: 0.6776 - val_accuracy: 0.7666\n",
            "Epoch 25/31\n",
            "137/137 [==============================] - 211s 2s/step - loss: 0.5694 - accuracy: 0.7983 - val_loss: 0.6759 - val_accuracy: 0.7733\n",
            "Epoch 26/31\n",
            "137/137 [==============================] - 230s 2s/step - loss: 0.5539 - accuracy: 0.8064 - val_loss: 0.6386 - val_accuracy: 0.7834\n",
            "Epoch 27/31\n",
            "137/137 [==============================] - 231s 2s/step - loss: 0.5444 - accuracy: 0.8101 - val_loss: 0.6529 - val_accuracy: 0.7767\n",
            "Epoch 28/31\n",
            " 25/137 [====>.........................] - ETA: 2:40 - loss: 0.5080 - accuracy: 0.8183"
          ]
        }
      ],
      "source": [
        "history = model_dropout.fit(X_train,\n",
        "                            y_train,\n",
        "                            batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                            epochs=TrainingConfig.EPOCHS,\n",
        "                            verbose=1,\n",
        "                            validation_split=.3,\n",
        "                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qlyf20uCGgj"
      },
      "outputs": [],
      "source": [
        "# Retrieve training results.\n",
        "train_loss = history.history[\"loss\"]\n",
        "train_acc  = history.history[\"accuracy\"]\n",
        "valid_loss = history.history[\"val_loss\"]\n",
        "valid_acc  = history.history[\"val_accuracy\"]\n",
        "\n",
        "plot_results([ train_loss, valid_loss ],\n",
        "            ylabel=\"Loss\",\n",
        "            ylim = [0.0, 5.0],\n",
        "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
        "            color=[\"g\", \"b\"]);\n",
        "\n",
        "plot_results([ train_acc, valid_acc ],\n",
        "            ylabel=\"Accuracy\",\n",
        "            ylim = [0.0, 1.0],\n",
        "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
        "            color=[\"g\", \"b\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ri1p_5DCHbL"
      },
      "outputs": [],
      "source": [
        "# Using the save() method, the model will be saved to the file system in the 'SavedModel' format.\n",
        "model_dropout.save('model_dropout')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKNqaL7zCJYb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "reloaded_model_dropout = models.load_model('model_dropout')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhQuGtG-CMqL"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = reloaded_model_dropout.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc*100:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwfSvU0CUXb"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(dataset, model):\n",
        "\n",
        "    class_names = ['airplane',\n",
        "                   'automobile',\n",
        "                   'bird',\n",
        "                   'cat',\n",
        "                   'deer',\n",
        "                   'dog',\n",
        "                   'frog',\n",
        "                   'horse',\n",
        "                   'ship',\n",
        "                   'truck' ]\n",
        "    num_rows = 3\n",
        "    num_cols = 6\n",
        "\n",
        "    # Retrieve a number of images from the dataset.\n",
        "    data_batch = dataset[0:num_rows*num_cols]\n",
        "\n",
        "    # Get predictions from model.\n",
        "    predictions = model.predict(data_batch)\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    num_matches = 0\n",
        "\n",
        "    for idx in range(num_rows*num_cols):\n",
        "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(data_batch[idx])\n",
        "\n",
        "        pred_idx = tf.argmax(predictions[idx]).numpy()\n",
        "        truth_idx = np.nonzero(y_test[idx])\n",
        "\n",
        "        title = str(class_names[truth_idx[0][0]]) + \" : \" + str(class_names[pred_idx])\n",
        "        title_obj = plt.title(title, fontdict={'fontsize':13})\n",
        "\n",
        "        if pred_idx == truth_idx:\n",
        "            num_matches += 1\n",
        "            plt.setp(title_obj, color='g')\n",
        "        else:\n",
        "            plt.setp(title_obj, color='r')\n",
        "\n",
        "        acc = num_matches/(idx+1)\n",
        "    print(\"Prediction accuracy: \", int(100*acc)/100)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwQfMPeyCVIz"
      },
      "outputs": [],
      "source": [
        "evaluate_model(X_test, reloaded_model_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv1ExPnxCX3z"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test dataset.\n",
        "predictions = reloaded_model_dropout.predict(X_test)\n",
        "\n",
        "# For each sample image in the test dataset, select the class label with the highest probability.\n",
        "predicted_labels = [np.argmax(i) for i in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_xzOD8QCaRr"
      },
      "outputs": [],
      "source": [
        "# Convert one-hot encoded labels to integers.\n",
        "y_test_integer_labels = tf.argmax(y_test, axis=1)\n",
        "\n",
        "# Generate a confusion matrix for the test dataset.\n",
        "cm = tf.math.confusion_matrix(labels=y_test_integer_labels, predictions=predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap.\n",
        "plt.figure(figsize=[14, 7])\n",
        "import seaborn as sn\n",
        "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 12})\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}